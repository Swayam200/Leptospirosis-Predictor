{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b27298-ecf2-4b18-99b4-4dcec36664c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netCDF4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: cftime in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from netCDF4) (1.6.4.post1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from netCDF4) (2025.1.31)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from netCDF4) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install netCDF4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6761d36f-1e10-4671-9539-01b6cbfbd72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping years: 17\n",
      "Aligned lengths:\n",
      "Length of time_flat: 17650080\n",
      "Length of lat_flat: 17650080\n",
      "Length of lon_flat: 17650080\n",
      "Length of t2m_flat: 17650080\n",
      "Length of d2m_flat: 17650080\n",
      "Length of tp_flat: 17650080\n",
      "Combined CSV file saved to weather_processing.csv\n"
     ]
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from netCDF4 import num2date\n",
    "\n",
    "# Paths to the NetCDF files\n",
    "file_path_0 = 'weather_data/data_0.nc'\n",
    "file_path_1 = 'weather_data/data_1.nc'\n",
    "\n",
    "# Load the first NetCDF file (data_0.nc)\n",
    "dataset_0 = nc.Dataset(file_path_0)\n",
    "\n",
    "# Extract variables from data_0.nc\n",
    "valid_time_0 = dataset_0.variables['valid_time'][:]\n",
    "latitude_0 = dataset_0.variables['latitude'][:]\n",
    "longitude_0 = dataset_0.variables['longitude'][:]\n",
    "t2m_0 = dataset_0.variables['t2m'][:]  # 2-meter temperature\n",
    "d2m_0 = dataset_0.variables['d2m'][:]  # 2-meter dewpoint temperature\n",
    "\n",
    "# Load the second NetCDF file (data_1.nc)\n",
    "dataset_1 = nc.Dataset(file_path_1)\n",
    "\n",
    "# Extract variables from data_1.nc\n",
    "valid_time_1 = dataset_1.variables['valid_time'][:]\n",
    "latitude_1 = dataset_1.variables['latitude'][:]\n",
    "longitude_1 = dataset_1.variables['longitude'][:]\n",
    "tp_1 = dataset_1.variables['tp'][:]  # Total precipitation\n",
    "\n",
    "# Convert valid_time to datetime objects and extract the year\n",
    "reference_date_0 = dataset_0.variables['valid_time'].units  # e.g., \"seconds since 1970-01-01\"\n",
    "dates_0 = num2date(valid_time_0, reference_date_0)\n",
    "valid_time_0_year = np.array([date.year for date in dates_0])\n",
    "\n",
    "reference_date_1 = dataset_1.variables['valid_time'].units\n",
    "dates_1 = num2date(valid_time_1, reference_date_1)\n",
    "valid_time_1_year = np.array([date.year for date in dates_1])\n",
    "\n",
    "# Find overlapping years\n",
    "common_years = np.intersect1d(valid_time_0_year, valid_time_1_year)\n",
    "\n",
    "# Print the number of overlapping years\n",
    "print(f\"Number of overlapping years: {len(common_years)}\")\n",
    "if len(common_years) == 0:\n",
    "    raise ValueError(\"No overlapping years found between the two files!\")\n",
    "\n",
    "# Filter data_0.nc to include only common years\n",
    "mask_0 = np.isin(valid_time_0_year, common_years)\n",
    "t2m_0_filtered = t2m_0[mask_0]\n",
    "d2m_0_filtered = d2m_0[mask_0]\n",
    "\n",
    "# Filter data_1.nc to include only common years\n",
    "mask_1 = np.isin(valid_time_1_year, common_years)\n",
    "tp_1_filtered = tp_1[mask_1]\n",
    "\n",
    "# Flatten the multidimensional arrays\n",
    "time_flat = np.repeat(common_years, len(latitude_0) * len(longitude_0))\n",
    "lat_flat = np.tile(np.repeat(latitude_0, len(longitude_0)), len(common_years))\n",
    "lon_flat = np.tile(longitude_0, len(common_years) * len(latitude_0))\n",
    "\n",
    "# Flatten the variables\n",
    "t2m_flat = t2m_0_filtered.flatten()\n",
    "d2m_flat = d2m_0_filtered.flatten()\n",
    "tp_flat = tp_1_filtered.flatten()\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "min_length = min(len(time_flat), len(lat_flat), len(lon_flat), len(t2m_flat), len(d2m_flat), len(tp_flat))\n",
    "time_flat = time_flat[:min_length]\n",
    "lat_flat = lat_flat[:min_length]\n",
    "lon_flat = lon_flat[:min_length]\n",
    "t2m_flat = t2m_flat[:min_length]\n",
    "d2m_flat = d2m_flat[:min_length]\n",
    "tp_flat = tp_flat[:min_length]\n",
    "\n",
    "# Verify the lengths again\n",
    "print(\"Aligned lengths:\")\n",
    "print(\"Length of time_flat:\", len(time_flat))\n",
    "print(\"Length of lat_flat:\", len(lat_flat))\n",
    "print(\"Length of lon_flat:\", len(lon_flat))\n",
    "print(\"Length of t2m_flat:\", len(t2m_flat))\n",
    "print(\"Length of d2m_flat:\", len(d2m_flat))\n",
    "print(\"Length of tp_flat:\", len(tp_flat))\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Year': time_flat,\n",
    "    'Latitude': lat_flat,\n",
    "    'Longitude': lon_flat,\n",
    "    'T2M': t2m_flat,  # 2-meter temperature\n",
    "    'D2M': d2m_flat,  # 2-meter dewpoint temperature\n",
    "    'TP': tp_flat     # Total precipitation\n",
    "})\n",
    "\n",
    "# Export to CSV\n",
    "output_csv_path = 'weather_processing.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV file saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77bd8a63-8acc-4a15-8185-9565e6ed6cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: reverse_geocode in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.6.5)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from geopy) (2.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from reverse_geocode) (1.26.4)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from reverse_geocode) (1.14.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy reverse_geocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9044c-9ef6-442f-b5a3-4c994c1115fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define input and output paths\n",
    "input_csv_path = 'weather_processing.csv'  # Your input file path\n",
    "output_csv_path = 'weather_processing_w_countries.csv'  # Your output file path\n",
    "\n",
    "# Define the valid countries and their RegionCodes with centroids\n",
    "valid_countries = {\n",
    "    \"Austria\": {\"code\": \"AT\", \"centroid\": (47.5162, 14.5501)},\n",
    "    \"Belgium\": {\"code\": \"BE\", \"centroid\": (50.5039, 4.4699)},\n",
    "    \"Bulgaria\": {\"code\": \"BG\", \"centroid\": (42.7339, 25.4858)},\n",
    "    \"Cyprus\": {\"code\": \"CY\", \"centroid\": (35.1264, 33.4299)},\n",
    "    \"Czechia\": {\"code\": \"CZ\", \"centroid\": (49.8175, 15.4730)},\n",
    "    \"Germany\": {\"code\": \"DE\", \"centroid\": (51.1657, 10.4515)},\n",
    "    \"Denmark\": {\"code\": \"DK\", \"centroid\": (56.2639, 9.5018)},\n",
    "    \"Estonia\": {\"code\": \"EE\", \"centroid\": (58.5953, 25.0136)},\n",
    "    \"Greece\": {\"code\": \"EL\", \"centroid\": (39.0742, 21.8243)},\n",
    "    \"Spain\": {\"code\": \"ES\", \"centroid\": (40.4637, -3.7492)},\n",
    "    \"Finland\": {\"code\": \"FI\", \"centroid\": (61.9241, 25.7482)},\n",
    "    \"Hungary\": {\"code\": \"HU\", \"centroid\": (47.1625, 19.5033)},\n",
    "    \"Ireland\": {\"code\": \"IE\", \"centroid\": (53.1424, -7.6921)},\n",
    "    \"Italy\": {\"code\": \"IT\", \"centroid\": (41.8719, 12.5674)},\n",
    "    \"Lithuania\": {\"code\": \"LT\", \"centroid\": (55.1694, 23.8813)},\n",
    "    \"Luxembourg\": {\"code\": \"LU\", \"centroid\": (49.8153, 6.1296)},\n",
    "    \"Latvia\": {\"code\": \"LV\", \"centroid\": (56.8796, 24.6032)},\n",
    "    \"Malta\": {\"code\": \"MT\", \"centroid\": (35.9375, 14.3754)},\n",
    "    \"Netherlands\": {\"code\": \"NL\", \"centroid\": (52.1326, 5.2913)},\n",
    "    \"Poland\": {\"code\": \"PL\", \"centroid\": (51.9194, 19.1451)},\n",
    "    \"Portugal\": {\"code\": \"PT\", \"centroid\": (39.3999, -8.2245)},\n",
    "    \"Romania\": {\"code\": \"RO\", \"centroid\": (45.9432, 24.9668)},\n",
    "    \"Sweden\": {\"code\": \"SE\", \"centroid\": (60.1282, 18.6435)},\n",
    "    \"Slovenia\": {\"code\": \"SI\", \"centroid\": (46.1512, 14.9955)},\n",
    "    \"Slovakia\": {\"code\": \"SK\", \"centroid\": (48.6690, 19.6990)},\n",
    "    \"United Kingdom\": {\"code\": \"UK\", \"centroid\": (55.3781, -3.4360)},\n",
    "    \"France\": {\"code\": \"FR\", \"centroid\": (46.2276, 2.2137)},\n",
    "    \"Croatia\": {\"code\": \"HR\", \"centroid\": (45.1000, 15.2000)},\n",
    "    \"Iceland\": {\"code\": \"IS\", \"centroid\": (64.9631, -19.0208)},\n",
    "}\n",
    "\n",
    "\n",
    "# Precompute centroids as NumPy arrays\n",
    "centroids = np.radians(np.array([info[\"centroid\"] for info in valid_countries.values()]))\n",
    "country_codes = np.array([info[\"code\"] for info in valid_countries.values()])\n",
    "country_names = np.array(list(valid_countries.keys()))\n",
    "\n",
    "# Function to calculate Haversine distance\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in kilometers\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    \n",
    "    a = np.sin(dphi / 2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Vectorized function to find the nearest country\n",
    "def vectorized_find_nearest_country(latitudes, longitudes):\n",
    "    points = np.radians(np.column_stack((latitudes, longitudes)))\n",
    "    distances = haversine_distance(\n",
    "        points[:, 0][:, None], points[:, 1][:, None],\n",
    "        centroids[:, 0], centroids[:, 1]\n",
    "    )\n",
    "    nearest_indices = np.argmin(distances, axis=1)\n",
    "    return country_codes[nearest_indices], country_names[nearest_indices]\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 5000\n",
    "\n",
    "# Initialize counters for debugging\n",
    "total_rows_processed = 0\n",
    "total_rows_filtered = 0\n",
    "errors_encountered = 0\n",
    "\n",
    "# Define European boundaries\n",
    "europe_latitude_range = (34.0, 71.0)\n",
    "europe_longitude_range = (-25.0, 40.0)\n",
    "\n",
    "# Count total rows for progress bar\n",
    "print(\"Counting total rows...\")\n",
    "total_rows = sum(1 for _ in open(input_csv_path)) - 1\n",
    "print(f\"Total rows to process: {total_rows}\")\n",
    "\n",
    "# Initialize progress bar\n",
    "progress_bar = tqdm(total=total_rows, desc=\"Processing Rows\", unit=\"row\")\n",
    "\n",
    "# Open the output file in write mode\n",
    "output_file = open(output_csv_path, 'w')\n",
    "header_written = False\n",
    "\n",
    "try:\n",
    "    for chunk_number, chunk in enumerate(pd.read_csv(input_csv_path, chunksize=chunk_size)):\n",
    "        try:\n",
    "            # Print debugging info for the chunk\n",
    "            print(f\"\\nProcessing chunk {chunk_number + 1}\")\n",
    "            print(f\"Chunk size before filtering: {len(chunk)}\")\n",
    "            \n",
    "            # Check for NaN values before filtering\n",
    "            nan_counts = chunk[['Latitude', 'Longitude']].isna().sum()\n",
    "            print(f\"NaN values in chunk: {nan_counts}\")\n",
    "            \n",
    "            # Filter for Europe with additional error checking\n",
    "            valid_coords = (\n",
    "                chunk['Latitude'].notna() &\n",
    "                chunk['Longitude'].notna() &\n",
    "                (chunk['Latitude'] >= europe_latitude_range[0]) &\n",
    "                (chunk['Latitude'] <= europe_latitude_range[1]) &\n",
    "                (chunk['Longitude'] >= europe_longitude_range[0]) &\n",
    "                (chunk['Longitude'] <= europe_longitude_range[1])\n",
    "            )\n",
    "            \n",
    "            chunk = chunk[valid_coords]\n",
    "            \n",
    "            print(f\"Chunk size after filtering: {len(chunk)}\")\n",
    "            total_rows_filtered += len(chunk)\n",
    "            \n",
    "            if chunk.empty:\n",
    "                progress_bar.update(chunk_size)\n",
    "                total_rows_processed += chunk_size\n",
    "                continue\n",
    "            \n",
    "            # Apply the vectorized function to find the nearest country\n",
    "            chunk['Country Code'], chunk['Country Name'] = vectorized_find_nearest_country(\n",
    "                chunk['Latitude'].values, chunk['Longitude'].values\n",
    "            )\n",
    "            \n",
    "            # Write the chunk to the output file\n",
    "            if not header_written:\n",
    "                chunk.to_csv(output_file, index=False)\n",
    "                header_written = True\n",
    "            else:\n",
    "                chunk.to_csv(output_file, index=False, header=False)\n",
    "            \n",
    "            # Update counters\n",
    "            total_rows_processed += len(chunk)\n",
    "            progress_bar.update(len(chunk))\n",
    "            \n",
    "        except Exception as chunk_error:\n",
    "            errors_encountered += 1\n",
    "            print(f\"\\nError in chunk {chunk_number + 1}: {chunk_error}\")\n",
    "            print(\"Last 5 rows being processed:\")\n",
    "            print(chunk.tail())\n",
    "            \n",
    "            # Continue with next chunk instead of stopping\n",
    "            continue\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCritical error encountered: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Print summary statistics\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total rows processed: {total_rows_processed}\")\n",
    "    print(f\"Total rows filtered (in Europe): {total_rows_filtered}\")\n",
    "    print(f\"Total errors encountered: {errors_encountered}\")\n",
    "    \n",
    "    # Close the output file and progress bar\n",
    "    output_file.close()\n",
    "    progress_bar.close()\n",
    "\n",
    "print(\"\\nProcessing complete. Updated CSV file saved to output_with_countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1ee6a4-348b-472d-9d61-a1c3d9ecbd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Country Distribution:\n",
      "                Count  Percentage\n",
      "Country Name                     \n",
      "Finland         62713       15.38\n",
      "Denmark         41769       10.24\n",
      "Sweden          38131        9.35\n",
      "Cyprus          32793        8.04\n",
      "Romania         27387        6.72\n",
      "Greece          17935        4.40\n",
      "Italy           17816        4.37\n",
      "Malta           16473        4.04\n",
      "Bulgaria        16167        3.96\n",
      "France          15232        3.74\n",
      "Estonia         13702        3.36\n",
      "Latvia          13447        3.30\n",
      "Lithuania       12240        3.00\n",
      "United Kingdom  11526        2.83\n",
      "Poland           8483        2.08\n",
      "Netherlands      8347        2.05\n",
      "Spain            8194        2.01\n",
      "Germany          7990        1.96\n",
      "Luxembourg       6239        1.53\n",
      "Czechia          5627        1.38\n",
      "Hungary          5593        1.37\n",
      "Croatia          5151        1.26\n",
      "Belgium          4998        1.23\n",
      "Slovakia         4114        1.01\n",
      "Austria          3859        0.95\n",
      "Slovenia         1887        0.46\n",
      "\n",
      "Total rows in output: 407813\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the output file\n",
    "df = pd.read_csv(\"weather_processing_w_countries.csv\")\n",
    "\n",
    "# Get country distribution\n",
    "country_counts = df['Country Name'].value_counts()\n",
    "\n",
    "# Calculate percentages\n",
    "country_percentages = (country_counts / len(df) * 100).round(2)\n",
    "\n",
    "# Combine counts and percentages\n",
    "country_stats = pd.DataFrame({\n",
    "    'Count': country_counts,\n",
    "    'Percentage': country_percentages\n",
    "})\n",
    "\n",
    "# Sort by count in descending order\n",
    "print(\"\\nCountry Distribution:\")\n",
    "print(country_stats)\n",
    "\n",
    "print(f\"\\nTotal rows in output: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ad5fa-65c1-44b3-99c9-f557aa721bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
